{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross referencing Google location history with MOH COVID-19 exposure locations\n",
    "## Important notice!\n",
    "> This code was **not** created with accuracy in mind, this is a POC and I really wanted to try my hand with pandas and used COVID-19 as an excuse. Please **do not** rely on the results.\n",
    "I know for a fact that some locations that are marked as \"clear\" (location history indicated not in proximity of location) also have additional date and time information in the comment field.\n",
    "\n",
    "## Notes\n",
    "- This code calculates location history distance from exposure locations without any distance considuration.\n",
    "- The code is not just **not optimized**, it bad, slow and unreadable.\n",
    "- First time using pandas, didn't read the documentation. Also, not a programmer.\n",
    "- It does work...\n",
    "\n",
    "## Known bugs\n",
    "- MOH timestamp fields are untrustworthy, therefore a combination of the timestamp and a stayTimes string  is used.\n",
    "- If a stayTimes string is ends before it begins (such as \"09:00 - 02:00\") no corrections are made to the date and location is ignored.\n",
    "- If the stayTimes string is \"לא ידוע\" (\"Unknown\"), there is not attempt done to select the full time range.\n",
    "- MOH like to add additional exposure times in the comment field, I ignore them.\n",
    "\n",
    "## Testing\n",
    "With the supplied myData.json and govData.json, there should be 1169 cases of unknown locations maked orange on the map. Out of the 132 known locations, 130 should be ok (green) and 2 should be too close to an exposure location and marked with a red marker on the map (one in Ra'anana\n",
    " and another at the Ruppin academic center).\n",
    "\n",
    "## Logic behind the code\n",
    "    None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime as dt\n",
    "import geopy.distance\n",
    "import json\n",
    "import os\n",
    "from ipywidgets import IntSlider\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "from ipyleaflet import Map, FullScreenControl, Marker, basemaps, AwesomeIcon\n",
    "import itertools\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# Make page wider, better view of tabular data\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asset acquisition\n",
    "This will download the latest MOH exposure locations file. Also, make sure the \"Location History.json\" file from google\n",
    "is located in the same directory under the name \"myData.json\". If I did not forget, there should be a sample \"myData.json\" file in the repository to run this notebook without actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 exposure locations from MOH\n",
    "GOV_CASE_LOC_URL = \"https://gisweb.azureedge.net/Points.json\"\n",
    "\n",
    "def downloadFile(url, path):\n",
    "    \"\"\"\n",
    "    Download a file and save it.\n",
    "    \"\"\"\n",
    "    print(f\"Requesting {url}\")\n",
    "    res = requests.get(url, stream=True)\n",
    "    length = res.headers.get(\"content-length\")\n",
    "    print(f\"Downloading\")\n",
    "    with open(path, 'wb') as f:\n",
    "        if length is None:\n",
    "            print(\"No content length.\")\n",
    "            f.write(res.content)\n",
    "        else:\n",
    "            length = int(length)\n",
    "            dl_length = 0\n",
    "            print(f\"Total size: {length}\")\n",
    "            for data in res.iter_content(chunk_size=4096):\n",
    "                dl_length += len(data)\n",
    "                f.write(data)\n",
    "            print(f\"Downloaded: {dl_length}\")\n",
    "\n",
    "# Uncomment to download newest MOH dataset\n",
    "#downloadFile(GOV_CASE_LOC_URL, 'govData.json')\n",
    "if not os.path.isfile('myData.json'):\n",
    "    # Copy the \"Location History.json\" file from Google takeout to here, rename it \"myData.json\"\n",
    "    raise Exception(\"Missing location history file (myData.json). \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google data normalization \n",
    "Load the google location history and put it into a known DataFrame with a known format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Transform Google location history data to a known dataframe\n",
    "# Note to self: Google Location History files are big, dont load with \"pd.load_json\"\n",
    "loch_data = json.load(open(\"myData.json\", \"r\"))\n",
    "loch_df = pd.DataFrame.from_dict(loch_data)\n",
    "print(f\"Number of Google location history points loaded: {loch_df.shape[0]:,}.\")\n",
    "\n",
    "# Extract timestamp, long, lat, accuracy and datetime from Location History.\n",
    "# For future use and debugging, the timestamp is also converted to a datetime.\n",
    "# Note, timestamp is in mS, long and lat must be devided by 1e7.\n",
    "loch_df['timestamp'] = loch_df.locations.map(lambda x: int(x['timestampMs'])) # Not really used\n",
    "loch_df['longitude'] = loch_df.locations.map(lambda x: x['longitudeE7']/1e7)\n",
    "loch_df['latitude'] = loch_df.locations.map(lambda x: x['latitudeE7']/1e7)\n",
    "loch_df['accuracy'] = loch_df.locations.map(lambda x: x['accuracy'])\n",
    "loch_df['datetime'] = loch_df.timestamp.map(lambda x: dt.datetime.fromtimestamp(x/1000))\n",
    "\n",
    "# Drop the rest of the dataset that is now irrelevent.\n",
    "loch_df= loch_df.drop(labels=['locations'], axis=1, inplace=False)\n",
    "\n",
    "# Display subset of data\n",
    "print(f\"Timeline: from {loch_df.datetime.min().strftime('%d/%m/%Y %H:%M')} to {loch_df.datetime.max().strftime('%d/%m/%Y %H:%M')}\")\n",
    "print(f\"Accuracy: min {loch_df.accuracy.min()}, max {loch_df.accuracy.max()}\")\n",
    "display(loch_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOH data normalization \n",
    "A very band and lazy attempt at normalizing the MOH dataset, there are so many edge cases.\n",
    "In order to get a (maybe?) correct duration time, the date component of the timestamp is used with a strptime parsed string of the \n",
    "time component. The only edge case which is handled here is when the end datetime time component is 00:00, I manualy add a delta of 1 day to the datetime (as the MOH dont advance the date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def _convertTime(time, index, start_timestamp_ms, end_timestamp_ms):\n",
    "    \"\"\"\n",
    "    A horrible function to try and convert the disguising MOH time data to something useful.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_time = dt.datetime.strptime(time.split('-')[index].strip(), \"%H:%M\").time()\n",
    "        parsed_start_date = dt.datetime.fromtimestamp(start_timestamp_ms/1000).date()\n",
    "        parsed_end_date = dt.datetime.fromtimestamp(end_timestamp_ms/1000).date()\n",
    "        if index == 0:\n",
    "            parsed_date = parsed_start_date\n",
    "        else:\n",
    "            parsed_date = parsed_end_date\n",
    "        \n",
    "        # Dumb MOH programmers use 00:00 to denote midnight without changing the date\n",
    "        # So, if we are looking for the end date, and the time is 00:00 and the dates did not change, fix it.\n",
    "        # This still does not fix the \"09:00 - 02:00\" problem.\n",
    "        if (index == 1) and (parsed_time == dt.time(0,0)) and (parsed_start_date == parsed_end_date):\n",
    "            delta = dt.timedelta(days=1)\n",
    "        else:\n",
    "            delta = dt.timedelta(days=0)\n",
    "        \n",
    "        return dt.datetime.combine(parsed_date + delta, parsed_time)\n",
    "    except Exception as ex:\n",
    "        # A date of 01/01/3000 marks a parse error.\n",
    "        return dt.datetime(3000,1,1,0,0,0)\n",
    "\n",
    "# Transform MOH data to a known dataframe\n",
    "moh_data = json.load(open(\"govData.json\",  \"r\")) \n",
    "moh_df = pd.DataFrame.from_dict(moh_data)  \n",
    "print(f\"Number of MOH points loaded: {moh_df.shape[0]:,}.\")\n",
    "\n",
    "# Extract id, long, lat, type, name, place, comments, fixed format times\n",
    "# Not exactly sure what \"type\" is, assuming will allow polygons/shapes in the future.\n",
    "moh_df['id'] = moh_df['features'].map(lambda x: x['id'])\n",
    "moh_df['longitude'] = moh_df['features'].map(lambda x: x['geometry']['coordinates'][0])\n",
    "moh_df['latitude'] = moh_df['features'].map(lambda x: x['geometry']['coordinates'][1])\n",
    "moh_df['type'] = moh_df['features'].map(lambda x: x['geometry']['type'])\n",
    "moh_df['iname'] = moh_df['features'].map(lambda x: x['properties']['Name'])\n",
    "moh_df['place'] = moh_df['features'].map(lambda x: x['properties']['Place'])\n",
    "moh_df['comments'] = moh_df['features'].map(lambda x: x['properties']['Comments'])\n",
    "# Note to self: The time convertion can be done better, much much better.\n",
    "moh_df['start_datetime'] = moh_df['features'].map(lambda x: _convertTime(x['properties']['stayTimes'], 0, x['properties']['fromTime'], x['properties']['toTime']))\n",
    "moh_df['end_datetime'] = moh_df['features'].map(lambda x: _convertTime(x['properties']['stayTimes'], 1, x['properties']['fromTime'], x['properties']['toTime']))\n",
    "# Debug datetime columns, not used for any referencing.\n",
    "moh_df['debug_stayTimes'] = moh_df['features'].map(lambda x: x['properties']['stayTimes'])\n",
    "moh_df['debug_fromTime'] = moh_df['features'].map(lambda x: dt.datetime.fromtimestamp(x['properties']['fromTime']/1000))\n",
    "moh_df['debug_toTime'] = moh_df['features'].map(lambda x: dt.datetime.fromtimestamp(x['properties']['fromTime']/1000))\n",
    "\n",
    "# Drop the rest of the dataset\n",
    "moh_df = moh_df.drop(labels=['features'], axis=1, inplace=False)\n",
    "\n",
    "# Dispay subset of data and count and subset of bad time information\n",
    "print(f\"Timeline: from {moh_df.start_datetime.min().strftime('%d/%m/%Y %H:%M')} to {moh_df.end_datetime[moh_df.start_datetime < dt.datetime(3000,1,1,0,0,0)].max().strftime('%d/%m/%Y %H:%M')}\")  \n",
    "bad_datetimes = moh_df[moh_df.start_datetime >= moh_df.end_datetime]\n",
    "no_bad_datetimes = bad_datetimes.shape[0]\n",
    "unknown_datetimes = moh_df[moh_df.start_datetime == dt.datetime(3000,1,1,0,0,0)]\n",
    "no_unknown_datetimes = unknown_datetimes.shape[0]\n",
    "print(f\"Bad start/end datetimes: {no_bad_datetimes}\") \n",
    "print(f\"Unknown datetimes: {no_unknown_datetimes}\") \n",
    "print(f\"Total valid datapoints: {moh_df.shape[0] - no_bad_datetimes - no_unknown_datetimes}\")\n",
    "print(\"-\"*40)\n",
    "print(\"Subset of valid datapoints:\")\n",
    "display(moh_df)\n",
    "print(\"Subset of bad start/end datapoints:\")\n",
    "display(bad_datetimes)\n",
    "print(\"Subset of unknown datetime datapoints:\")\n",
    "display(unknown_datetimes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a slice of only the relevant  dates out of the location history DataFrame.\n",
    "loch_slice = loch_df[(loch_df.datetime >=  moh_df.start_datetime.min()) & (loch_df.datetime <= moh_df.end_datetime[moh_df.end_datetime < dt.datetime(3000,1,1,0,0,0)].max())]\n",
    "print(f\"Cross referencing with location history from {loch_slice.datetime.min().strftime('%d/%m/%Y %H:%M')} to {loch_slice.datetime.max().strftime('%d/%m/%Y %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The super duper deoptimized cross referencing loops\n",
    "I cant really explain this, but this is what I think I did:\n",
    "1. Create master results DataFrame.\n",
    "2. For each row of the MOH exposure points:\n",
    "    - Do some time sanity checks, if time is wierd, put it aside.\n",
    "    - Get the time relevant slice of location history.\n",
    "    - For each location history point, mesure the distance and add to a private dataframe\n",
    "    - Take the results with minimum distance and the minimum accuracy and combine them into a sngle result row in the master results DataFrame.\n",
    "3. Display subset of master results DataFrame.\n",
    "\n",
    "> Note: There is no reason to create the internal DataFrame, I did it this way so later I could use the DataFrames as sample data to play with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "TIME_BUFFER = dt.timedelta(minutes=0)\n",
    "# Counters for bad datapoints\n",
    "# Note: These were counters, now they are lists.\n",
    "counters = { 'unknown_datetime' : [], 'bad_datetime' : [], 'missing_results' : [], 'debug_timestamp' : [], 'totally_failed' : []}\n",
    "# Total dataset size\n",
    "frame_size = moh_df.shape[0]\n",
    "\n",
    "# Do I really need to create this DataFrame? No.\n",
    "results_df = pd.DataFrame(columns=['incident_id', 'incident_time', 'incident_location', 'incident_name','incident_place', 'incident_comments', 'min_distance_location','min_distance_accuracy', 'min_distance_distance', 'min_accuracy_accuracy'])\n",
    "for inc_idx, inc_row in moh_df.iterrows():\n",
    "    # Everything is in a try, whay? because MOD data can lie!\n",
    "    try:\n",
    "        # Time buffers! may be useful in the future\n",
    "        search_start_datetime = inc_row.start_datetime - TIME_BUFFER\n",
    "        search_end_datetime = inc_row.end_datetime + TIME_BUFFER\n",
    "        \n",
    "        # Some sanity checks\n",
    "        if inc_row.start_datetime > inc_row.end_datetime:\n",
    "            counters['bad_datetime'].append(inc_row)\n",
    "            continue\n",
    "        if inc_row.start_datetime == dt.datetime(3000,1,1,0,0,0):\n",
    "            counters['unknown_datetime'].append(inc_row)\n",
    "            continue\n",
    "            # Select the whole timerange\n",
    "            search_start_datetime = datetime.combine(search_start_datetime.date(), dt.time(0,0))\n",
    "            search_end_datetime = datetime.combine(search_end_datetime.date() + dt.timedelta(days=1), dt.time(0,0))\n",
    "        # Debug, just to see how many timestams are *maybe* correct\n",
    "        if inc_row.debug_fromTime != inc_row.debug_toTime:\n",
    "            counters['debug_timestamp'].append(inc_row)\n",
    "\n",
    "        # Get all locations from location history that corrspond to the datetime\n",
    "        loch_results = loch_slice[(loch_slice.datetime >= search_start_datetime ) & (loch_slice.datetime <= search_end_datetime)]\n",
    "        if loch_results.empty:\n",
    "            # No results, manual verification :(\n",
    "            counters['missing_results'].append(inc_row)\n",
    "            continue\n",
    "\n",
    "        # We can only set a distance and index, but it's easy to run slicings on a clean DataFrame\n",
    "        measured_distances = pd.DataFrame(columns=['index','datetime', 'distance', 'latitude', 'longitude', 'accuracy'])\n",
    "        for loc_idx, loc_row in loch_results.iterrows():\n",
    "            distance = geopy.distance.geodesic((inc_row.longitude, inc_row.latitude),(loc_row.longitude, loc_row.latitude))\n",
    "            measured_distances = measured_distances.append({ 'index' : loc_idx,\n",
    "                                                           'datetime' : loc_row.datetime,\n",
    "                                                           'distance' : distance.km,\n",
    "                                                           'latitude' : loc_row.longitude,\n",
    "                                                           'longitude' : loc_row.latitude,\n",
    "                                                           'accuracy' : loc_row.accuracy}, ignore_index=True)\n",
    "        \n",
    "        min_distance_df = measured_distances.sort_values(by=['distance'], ).head(1)\n",
    "        min_accuracy_df = measured_distances.sort_values(by=['accuracy'], ).head(1)\n",
    "        \n",
    "        results_df = results_df.append({'incident_id' : inc_row.id, \n",
    "                                  'incident_time' : str(f'{inc_row.start_datetime.strftime(\"%d/%m/%Y %H:%M\")} - {inc_row.end_datetime.strftime(\"%d/%m/%Y %H:%M\")}'), \n",
    "                                  'incident_location' : (inc_row.latitude, inc_row.longitude), \n",
    "                                  'incident_name' : inc_row.iname,\n",
    "                                  'incident_place' : inc_row.place, \n",
    "                                  'incident_comments' : inc_row.comments,\n",
    "                                   # I dont know why (yet) but I needed to use here .item()\n",
    "                                  'min_distance_location' : (min_distance_df.latitude.item(), min_distance_df.longitude.item()), \n",
    "                                  'min_distance_accuracy' : min_distance_df.accuracy.item(),\n",
    "                                  'min_distance_distance' : min_distance_df.distance.item(),\n",
    "                                  'min_accuracy_location' : (min_accuracy_df.latitude.item(), min_accuracy_df.longitude.item()), \n",
    "                                  'min_accuracy_distance' : min_accuracy_df.distance.item(),\n",
    "                                  'min_accuracy_accuracy' : min_accuracy_df.accuracy.item()}, ignore_index=True)\n",
    "    except Exception as ex:\n",
    "        # Yep, the data lied.\n",
    "        print(f\"You have 1 new exception: {ex}\")\n",
    "        counters['totally_failed'].append(inc_row)\n",
    "    \n",
    "    # A poor mans progress meter.\n",
    "    if inc_idx % 100 == 0:\n",
    "        print(f\"\\rProcessed {inc_idx}/{frame_size} exposure incidents (estimate)\", end='')\n",
    "print(\" Done\")\n",
    "\n",
    "print(f\"Total incidents checked: {results_df.shape[0]}\")\n",
    "print(f\"Incidents with unknown time skipped: {len(counters['unknown_datetime'])}\")\n",
    "print(f\"Incidents with bad time skipped: {len(counters['bad_datetime'])}\")\n",
    "print(f\"Incidents with missing results: {len(counters['missing_results'])}\")\n",
    "print(f\"Incidents witch just failed for some reason: {len(counters['totally_failed'])}\")\n",
    "print(f\"Debug timestamp incidents: {len(counters['debug_timestamp'])}\")\n",
    "# Show some results    \n",
    "display(results_df.sort_values(by=['min_distance_distance']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Display a map\n",
    "Display a map swith all MOH locations on it.\n",
    "    - Red icons - Maybe exposed.\n",
    "    - Orange icons - No cross-referencing done.\n",
    "    - Green - Probably not exposed (again, hidden additional time data in the comments field).\n",
    "\n",
    "Due to not grouping of the markers (as I want to see them all with their colors) the map is **very** sluggish. I did not intent on adding a map, but it's Jupyter and I can. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CRITICAL_DISTANCE_KM = 1.0\n",
    "# Create the map\n",
    "m = Map(zoom=7, center=[31.771959, 35.217018],scroll_wheel_zoom=True,basemap = basemaps.OpenStreetMap.Mapnik)\n",
    "# Create the icons\n",
    "red_icon = AwesomeIcon(marker_color='red', name='exclamation-triangle')\n",
    "green_icon = AwesomeIcon(marker_color='green', name='check')\n",
    "orange_icon = AwesomeIcon(marker_color='orange', name='flag')\n",
    "\n",
    "# Add known locations markers\n",
    "known_count = len([m.add_layer(Marker(z_index_offset=50 if row.min_distance_distance < CRITICAL_DISTANCE_KM else 0,icon=red_icon if row.min_distance_distance < CRITICAL_DISTANCE_KM else green_icon,draggable=False, location=row.incident_location, title=f'מיקום: {row.incident_place}\\nהערות: {row.incident_comments}\\nזמן: {row.incident_time}\\nמרחק: {row.min_distance_distance:0.4f} ק\"מ\\nדיוק: {row.min_distance_accuracy}')) for idx, row in results_df.iterrows()])\n",
    "# Add unknown/bad/missing markers\n",
    "unknown_count = len([m.add_layer(Marker(z_index_offset=25, icon=orange_icon, draggable=False, location=(row.latitude, row.longitude), title=f'מיקום: {row.place}\\nהערות: {row.comments}\\nזמן: {row.start_datetime.strftime(\"%d/%m/%Y %H:%M\")} - {row.end_datetime.strftime(\"%d/%m/%Y %H:%M\")}')) for row in itertools.chain.from_iterable(counters.values())])\n",
    "# Alow full screen\n",
    "m.add_control(FullScreenControl())\n",
    "# Display some info \n",
    "print(f\"Added {known_count} known markers.\")\n",
    "print(f\"Added {unknown_count} unknown markers.\")\n",
    "print(f\"Total markers on map: {known_count + unknown_count}/{frame_size}\")\n",
    "display(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data and the map\n",
    "Saves the date and map, incase we want it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=['min_distance_distance']).to_html(dt.datetime.now().strftime(\"data/data%Y%m%d_%H%M.html\"))\n",
    "embed_minimal_html(dt.datetime.now().strftime(\"maps/map%Y%m%d_%H%M.html\"), views=[m], title='Exposure Map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
